{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# Lecture slides\n",
    "# https://blog.zhaytam.com/2018/08/15/implement-neural-network-backpropagation/\n",
    "# https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
    "# https://towardsdatascience.com/learning-rate-a6e7b84f1658\n",
    "# https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# from mpl_toolkits.mplot3d import axes3d, Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define our inputs and traget output according to XNOR Truth table\n",
    "inputs = np.array([[1,1],[1,-1],[-1,1], [-1,-1]])\n",
    "target_output = np.array([[1],[-1],[-1],[1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a 2-2-1 neural network, implies\n",
    "input_layer_neurons = 2\n",
    "hidden_layer_neurons = 2\n",
    "output_layer_neurons = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set the learning rate i.e., alpha and the number of iterations i.e., epochs:\n",
    "epochs = 10000\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Output Layer Weights:\n",
      " [[0.06421842]\n",
      " [0.44047024]]\n",
      "Initial Hidden Layer Weights:\n",
      " [[-0.31345188  0.09234885]\n",
      " [ 0.38342476  0.36175132]]\n",
      "Initial Output Layer Bias:\n",
      " [[-0.47409258]]\n",
      "Initial Hidden Layer Bias:\n",
      " [[0.27082469 0.1054798 ]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the weights and biases to have random values between âˆ’0.5 and 0.5:\n",
    "# weights\n",
    "hidden_layer_weights = np.random.uniform(-0.5, 0.5, size = (input_layer_neurons,hidden_layer_neurons))\n",
    "output_layer_weights = np.random.uniform(-0.5,0.5, size = (hidden_layer_neurons,output_layer_neurons))\n",
    "\n",
    "# bias:\n",
    "hidden_layer_bias = np.random.uniform(-0.5, 0.5,size = (1,hidden_layer_neurons))\n",
    "output_layer_bias = np.random.uniform(-0.5,0.5,size = (1,output_layer_neurons))\n",
    "\n",
    "print('Initial Output Layer Weights:\\n', output_layer_weights)\n",
    "print('Initial Hidden Layer Weights:\\n', hidden_layer_weights)\n",
    "print('Initial Output Layer Bias:\\n', output_layer_bias)\n",
    "print('Initial Hidden Layer Bias:\\n', hidden_layer_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training [[ 0.05024308]\n",
      " [-0.04022236]\n",
      " [ 0.04022236]\n",
      " [-0.05024308]]\n",
      "Final Output Layer Weights:\n",
      " [[5.17646069]\n",
      " [5.17700971]]\n",
      "Final Hidden Layer Weights:\n",
      " [[-3.3375171   3.24763671]\n",
      " [ 3.33742433 -3.24756198]]\n",
      "Final Output Layer Bias:\n",
      " [[-4.73072222]]\n",
      "Final Hidden Layer Bias:\n",
      " [[3.19698962 3.03164473]]\n",
      "Target Output:\n",
      " [[ 1]\n",
      " [-1]\n",
      " [-1]\n",
      " [ 1]]\n",
      "After Training:\n",
      " [[ 0.9826669 ]\n",
      " [-0.97623754]\n",
      " [-0.97615487]\n",
      " [ 0.98266698]]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Training the inputs\n",
    "\n",
    "# Step 2a:\n",
    "# Setting the corresponding activation(a1) for input_layer\n",
    "def sigmoid(x):\n",
    "    return (1 - np.exp(-x))/(1 + np.exp(-x))\n",
    "\n",
    "# We need the derivative of this for further calculations\n",
    "def derivative_of_sigmoid(x):\n",
    "#     derivative of sigmoid in terms of itslef is:\n",
    "    return 0.5 * (1 + x) * (1 - x)\n",
    "\n",
    "x1 = sigmoid(np.dot(inputs, hidden_layer_weights))\n",
    "x2 = sigmoid(np.dot(x1, output_layer_weights))\n",
    "print('Before Training', x2)\n",
    "\n",
    "for i in range(epochs):\n",
    "#     Step 2b:\n",
    "#     Feedforward\n",
    "    h_layer_a = np.dot(inputs, hidden_layer_weights)\n",
    "    h_layer_a = h_layer_a + hidden_layer_bias\n",
    "    output_hidden_layer = sigmoid(h_layer_a)\n",
    "    \n",
    "    o_layer_a = np.dot(output_hidden_layer, output_layer_weights)\n",
    "    o_layer_a = o_layer_a + output_layer_bias\n",
    "    pred_output = sigmoid(o_layer_a)\n",
    "\n",
    "#     Step 2c:\n",
    "#     Output error\n",
    "    final_error = target_output - pred_output\n",
    "    pred_output_delta = final_error * derivative_of_sigmoid(pred_output)\n",
    "#     pred_output_delta = np.multiply(final_error,derivative_of_sigmoid(pred_output))\n",
    "\n",
    "#     Step 2d:\n",
    "#     Backpropogating the error\n",
    "    h_layer_error = pred_output_delta.dot(output_layer_weights.T)\n",
    "    hidden_layer_output_delta = h_layer_error * derivative_of_sigmoid(output_hidden_layer)\n",
    "\n",
    "#     Step 2e:\n",
    "#     Updating weights and bias\n",
    "    output_layer_weights = output_layer_weights + output_hidden_layer.T.dot(pred_output_delta) * alpha\n",
    "    output_layer_bias = output_layer_bias + np.sum(pred_output_delta) * alpha\n",
    "    \n",
    "    hidden_layer_weights = hidden_layer_weights + inputs.T.dot(hidden_layer_output_delta) * alpha\n",
    "    hidden_layer_bias = hidden_layer_bias + np.sum(hidden_layer_output_delta) * alpha\n",
    "    \n",
    "#     alpha = alpha - (1/epochs)\n",
    "\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Mean Square Error')\n",
    "#     plt.plot(i, np.mean(final_error**2),'o')\n",
    "    \n",
    "#     plt.xlabel('Learning Rate')\n",
    "#     plt.ylabel('Mean Square Error')\n",
    "#     plt.plot(alpha, np.mean(final_error**2),'o')\n",
    "    \n",
    "    \n",
    "       \n",
    "# plt.savefig('Error vs Epochs.png')\n",
    "       \n",
    "print('Final Output Layer Weights:\\n', output_layer_weights)\n",
    "print('Final Hidden Layer Weights:\\n', hidden_layer_weights)\n",
    "print('Final Output Layer Bias:\\n', output_layer_bias)\n",
    "print('Final Hidden Layer Bias:\\n', hidden_layer_bias)\n",
    "\n",
    "print('Target Output:\\n', target_output)\n",
    "print('After Training:\\n', pred_output)\n",
    "# print('Predicted Output:\\n', pred_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
